#!/usr/bin/Rscript 
# Copyright (C) Abhishek Sheetal
# This file is part of HILDA methods project
#
# HILDA methods is free software: you can redistribute it and/or modify
# it under the terms of the GNU General Public License as published by
# the Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# HILDA methods is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with HILDA methods If not, see <http://www.gnu.org/licenses/>.
#
# This is the model query file. It does performance testing of the models and
# model query for explanations
#
suppressPackageStartupMessages({
  library(Rmisc)
  library(readxl)
  library(data.table)
  library(haven)
  library(finalfit) 
  library(dplyr)
  library(naniar)
  library(ggplot2)
  library(missRanger)
  library(rpart)
  library(rpart.plot)
  library(filenamer)
  library(caTools)
  library(DALEX)
  library(ingredients)
  library(gridExtra)
  library(grid)
  library(ggpubr)
  library(glmnet)
  library(readr)
  library(mlr)
  library(caret)
  library(xgboost)
  library(ParBayesianOptimization)
  library(naivebayes)
  library(pROC)
  library(mltools)
  library(scales)
  library(xlsx)
  library(DALEXtra)
  library(flashlight)
  library(ggthemr)
  library(tidyr)
  library(kableExtra)
  library(latticeExtra)
  library(ggthemr)
  library(ggthemes)
  library(stringr)
  library(testit)
  library(hrbrthemes)
  library(rstan)
  library(rstanarm)
  library(brms)
  library(bayestestR)
  library(fastDummies)
  library(keras)
  library(stargazer)
  library(purrr)
  library(stringr)
  `%notin%` <- Negate(`%in%`)
})

ggthemr("light")

#load("/research/dataset/HILDA/HILDA_methods/xgb_bayes_completed_model_neurotic_TRUE_2022-02-11.RData")
#load("/research/dataset/HILDA/HILDA_methods/xgb_bayes_completed_model_neurotic_FALSE_2022-02-11.RData") #old model with original hyperparameters
#load("/research/dataset/HILDA/HILDA_methods/xgb_bayes_completed_model_neurotic_FALSE_2022-02-23.RData") #this is built using imputed data 
#removed _m0, split by users
load("/research/dataset/HILDA/HILDA_methods/xgb_bayes_completed_model_neurotic_FALSE_2022-03-15.RData")

master_file <- "/research/dataset/HILDA/HILDA_methods/hilda_vars_AL_v2.xlsx"

if (FALSE) { #if working interactively
  load("/research/dataset/HILDA/HILDA_methods/xgb_model_analysis_FALSE.RData")
}

####
SAVE_FILE <- paste(BASE_PATH, "/xgb_model_analysis_", COMPLETE, ".RData", sep="")

xgb_vic_image <- paste(BASE_PATH, "/xgb_vic_model_", COMPLETE, ".png", sep="")
xgb_vic_table <- paste(BASE_PATH, "/xgb_vic_model_", COMPLETE, ".tex", sep="")
xgb_vic_table_label <- "tab:xgb_vic_model"
xgb_vic_table_caption <- "Top predictors generated by XGBoost Model"
box_image <- paste(BASE_PATH, "/resamples_boxplot_model_", COMPLETE, ".png", sep="")
dot_image <- paste(BASE_PATH, "/resamples_dotplot_model_", COMPLETE, ".png", sep="")
avg_vic_table <- paste(BASE_PATH, "/avg_vic_model_", COMPLETE, ".tex", sep="")
avg_vic_table_label <- "tab:avg_vic_model"
avg_vic_table_caption <- "Combined average predictors of XGBoost and Naive Bayes models"
pairwise_interaction_table <- paste(BASE_PATH, "/pairwise_interaction_model_", COMPLETE, ".xlsx", sep="")
test_perf_xgb <- paste(BASE_PATH, "/xgb_unseen_test_model_", COMPLETE, ".png", sep="")
test_perf_lasso <- paste(BASE_PATH, "/lasso_unseen_test_model_", COMPLETE, ".png", sep="")
tm_lag_image <- paste(BASE_PATH, "/tm_lag_image_", COMPLETE, ".png", sep="")
pdp_file <- paste(BASE_PATH, "/pdp_image_", COMPLETE, ".png", sep="")
report_card_file <- paste(BASE_PATH, "/report_card_", COMPLETE, ".tex", sep="")
effect_image <- paste(BASE_PATH, "/effect_image_", COMPLETE, ".png", sep="")
lm_report_table <- paste(BASE_PATH, "/lm_report_table_", COMPLETE, ".tex", sep="")
tm6_response <- paste(BASE_PATH, "/tm6_response_", COMPLETE, ".png", sep="")
accuracy_plot <- paste(BASE_PATH, "/xgb_accuracy_", COMPLETE, ".png", sep="")
accuracy_plot_bayes <- paste(BASE_PATH, "/bayes_accuracy_", COMPLETE, ".png", sep="")

df.test.pre <- readRDS(TRAIN_DATA) %>%
  pluck("df.test.imputed") %>%
  as.data.frame() %>%
  filter(!is.na(get(MODEL_VAR)))

if (COMPLETE) {
  df.test <- na.omit(df.test.pre)
} else {
  df.test <- df.test.pre
}


#generated R-squares
{ 
  posteriors <- describe_posterior(model_bayes)
  # for a nicer table
  print_md(posteriors, digits = 2)
  bayes_tr2_res <- round(bayes_R2(model_bayes), 5)
  
  y.pred_bayes <- predict(object = model_bayes, newdata = df.test, cores = 6, summary = TRUE, robust = FALSE)
  bayes_r2_res <- cor(y.pred_bayes[,1], df.test$neurotic) ^ 2 %>%
    round(5)
  display_df <- data.frame(Predicted = y.pred_bayes[,1], Actual = as.vector(df.test[,MODEL_VAR]))
  options(digits = 2)
  title_text <- paste("Unseen Testing on ", MODEL_VAR, " MCMC Model (N = ", nrow(df.test), ") with Pearson Correlation", sep="")
  p <- display_df %>%
    ggplot(aes(x = Predicted, y = Actual)) +
    geom_point(alpha=0.5) +
    scale_y_continuous(
      limits = c(min(display_df$Actual), max(display_df$Actual)),
      labels = scales::number_format(accuracy = 0.01)) + 
    scale_x_continuous(
      limits = c(min(display_df$Predicted), max(display_df$Predicted))) +
    geom_smooth(method=lm, col = "red") +
    stat_cor(method = "spearman", 
             size = 5,
             digits = 3,
             aes(label = paste(r.label,..rr.label.., ..p.label.., sep = "~`,`~")), 
             label.x = 1) +
    #ggtitle(title_text) +
    xlab("Predicted neuroticism") +
    ylab("Actual neuroticism") +
    theme(text = element_text(size=40)) +
    theme_bw() 
  p
  ggsave(accuracy_plot_bayes, width = 6, height = 6, units =  "in", dpi=300, plot = p)
  
  # for XGB?
  y.pred_xgb <- predict(object = xgb.tuned.bayes, newdata = df.test, na.action = na.pass, type = "raw") 
  r2_xgb <- round(cor(df.test[,MODEL_VAR], y.pred_xgb) ^ 2, 5)
  display_df <- data.frame(Predicted = y.pred_xgb, Actual = as.vector(df.test[,MODEL_VAR]))
  options(digits = 2)
  title_text <- paste("Unseen Testing on ", MODEL_VAR, " XGB Model (N = ", nrow(df.test), ") with Pearson Correlation", sep="")
  p <- display_df %>%
    ggplot(aes(x = Predicted, y = Actual)) +
    geom_point(alpha=0.5) +
    scale_y_continuous(
      limits = c(min(display_df$Actual), max(display_df$Actual)),
      labels = scales::number_format(accuracy = 0.01)) + 
    scale_x_continuous(
      limits = c(min(display_df$Predicted), max(display_df$Predicted))) +
    geom_smooth(method=lm, col = "red") +
    stat_cor(method = "spearman", 
             size = 5,
             digits = 3,
             aes(label = paste(r.label,..rr.label.., ..p.label.., sep = "~`,`~")), 
             label.x = 1.5) +    
    xlab("Predicted neuroticism") +
    ylab("Actual neuroticism") +
    theme(text = element_text(size=40)) +
    theme_bw()  
  p
  ggsave(accuracy_plot, width = 6, height = 6, units =  "in", dpi=300, plot = p)
}

save.image(SAVE_FILE)

#print the result table
{
  report_card <- data.frame(M = c("MCMC Bayesian regression with multiple imputation",
                                  "XGBoost with imputation"),
                            R2_train = c(paste("Mean = ", bayes_tr2_res[1], " [95\\% CI = ", bayes_tr2_res[3], " : ", bayes_tr2_res[4], "]", sep=""),
                                         paste(round(getTrainPerf(xgb.tuned.bayes)[, "TrainRsquared"], 4))),
                            R2 = c(paste("Mean = ", bayes_r2_res),
                                   r2_xgb))
  report_card %>%
    kable(format="latex", 
          #longtable = TRUE,
          booktabs = TRUE, linesep = "",
          row.names=FALSE, 
          col.names = c("Model Description",   
                        "Accuracy on training (as R\\textsuperscript{2})",
                        "Accuracy on unseen testing (as R\\textsuperscript{2})"), escape = F,
          #caption = paste("Model testing on secret 10\\% of the dataset (", length(y.pred.xgb), " samples)", sep=""),
          caption = paste("Model statistics. Training on", nrow(df), "samples. Testing on", nrow(df.test), "samples."),
          label = paste0("tabreportcard", COMPLETE)) %>%
    #collapse_rows(columns = 1) %>%
    kable_styling(latex_options = c("striped", "scale_down"), font_size = 4) %>%
    save_kable(report_card_file, 
               keep_tex = TRUE)
}

save.image(SAVE_FILE)

{
  new_df <- readRDS(TRAIN_DATA) %>%
    pluck("df.train") %>%
    as.data.frame() %>%
    filter(!is.na(get(MODEL_VAR)))
  
  exp_model_xgb <- DALEX::explain(xgb.tuned.bayes,
                                  data = new_df[,INDEPENDANT_VARS],
                                  y = new_df[,MODEL_VAR],
                                  predict_function = function(m,x) {
                                    p <- predict(m, x, type = "raw", na.action = na.pass)
                                  })
  
  orig_independant_vars <- str_remove(INDEPENDANT_VARS, "_m.*|_1.*|_2.*|_3.*|_4.*")
  master_vars <- read_excel(master_file, sheet = "Wu Name") %>%
    select(c("WuVariable", "grouped")) %>%
    filter(WuVariable %notin% c(all_big5, "xwaveid"))
  idx <- match(orig_independant_vars, master_vars$WuVariable)
  assert("No mismatch", !anyNA(idx))
  orig_independant_vars_groups <- master_vars$grouped[idx]
  assert("No NA groups", !anyNA(orig_independant_vars_groups))
  names(INDEPENDANT_VARS) <- orig_independant_vars_groups
  vars_list <- split(unname(INDEPENDANT_VARS),names(INDEPENDANT_VARS))
  LABEL <- "Outcome variable: Neuroticism"
  grouped_xgb <- ingredients::feature_importance(exp_model_xgb,
                                                 variable_groups = vars_list,
                                                 loss_function = loss_root_mean_square,
                                                 label = LABEL,
                                                 type = "raw",
                                                 B=100, 
                                                 N=NULL)
  
  exp_model_xgb_imp <- DALEX::explain(xgb.tuned.bayes,
                                      data = df[,INDEPENDANT_VARS],
                                      y = df[,MODEL_VAR],
                                      predict_function = function(m,x) {
                                        p <- predict(m, x, type = "raw", na.action = na.pass)
                                      })
  
  grouped_xgb_imp <- ingredients::feature_importance(exp_model_xgb_imp,
                                                     variable_groups = vars_list,
                                                     loss_function = loss_root_mean_square,
                                                     label = LABEL,
                                                     type = "raw",
                                                     B=100, 
                                                     N=NULL)
  df_complete <- df[complete.cases(new_df), ]
  exp_model_xgb_complete <- DALEX::explain(xgb.tuned.bayes,
                                      data = df_complete[,INDEPENDANT_VARS],
                                      y = df_complete[,MODEL_VAR],
                                      predict_function = function(m,x) {
                                        p <- predict(m, x, type = "raw", na.action = na.pass)
                                      })
  grouped_xgb_complete <- ingredients::feature_importance(exp_model_xgb_complete,
                                                     variable_groups = vars_list,
                                                     loss_function = loss_root_mean_square,
                                                     label = LABEL,
                                                     type = "raw",
                                                     B=100, 
                                                     N=NULL)
  
  p <- plot(grouped_xgb) +
    ggtitle("Grouped importance of variables") +
    labs(subtitle = element_blank()) +
    ylab("Change in mean square error") +
    theme_bw() +
    theme(legend.position = "none", title = element_blank())
  p
  ggsave(paste0(BASE_PATH, "/xgb_vic_model_grouped_raw.png"), 
         width = 7, height = 6, units = "in", dpi = 300, plot = p)
  
  p <- plot(grouped_xgb_imp) +
    ggtitle("Grouped importance of variables") +
    labs(subtitle = element_blank()) +
    ylab("Change in mean square error") +
    theme_bw() +
    theme(legend.position = "none", title = element_blank())
  p
  ggsave(paste0(BASE_PATH, "/xgb_vic_model_grouped_imp.png"), 
         width = 7, height = 6, units = "in", dpi = 300, plot = p)
  
  p <- plot(grouped_xgb_complete) +
    ggtitle("Grouped importance of variables") +
    labs(subtitle = element_blank()) +
    ylab("Change in mean square error") +
    theme_bw() +
    theme(legend.position = "none", title = element_blank())
  p
  ggsave(paste0(BASE_PATH, "/xgb_vic_model_grouped_complete.png"), 
         width = 7, height = 6, units = "in", dpi = 300, plot = p)
  save.image(SAVE_FILE)
}
